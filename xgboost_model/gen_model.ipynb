{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Productionize maching learning model using weights.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.372423Z",
     "start_time": "2019-11-29T13:05:19.368383Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.381265Z",
     "start_time": "2019-11-29T13:05:19.375299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yibin/Documents/MachineLearningDeployment/xgboost_model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current working directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.522632Z",
     "start_time": "2019-11-29T13:05:19.384286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: config: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.533106Z",
     "start_time": "2019-11-29T13:05:19.526098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./config/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/config.yml\n",
    "data_downloaded_path: \"/data/VTI_downloaded.csv\" # specify the path of your downloaded file here\n",
    "data_processed_path: \"/data/VTI.csv\"             # specify the path of your processed file here\n",
    "stock_code: \"VTI\"                                # stock code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.551903Z",
     "start_time": "2019-11-29T13:05:19.537053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_downloaded_path': '/data/VTI_downloaded.csv',\n",
       " 'data_processed_path': '/data/VTI.csv',\n",
       " 'stock_code': 'VTI'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load config file\n",
    "import yaml\n",
    "\n",
    "with open(\"./config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data into right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.600941Z",
     "start_time": "2019-11-29T13:05:19.557291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-11-17</td>\n",
       "      <td>95.533783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>96.097031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-19</td>\n",
       "      <td>95.851738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>96.124275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-11-21</td>\n",
       "      <td>96.633034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  adj_close\n",
       "0  2014-11-17  95.533783\n",
       "1  2014-11-18  96.097031\n",
       "2  2014-11-19  95.851738\n",
       "3  2014-11-20  96.124275\n",
       "4  2014-11-21  96.633034"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(cwd+config['data_downloaded_path'])\n",
    "\n",
    "# Remove unneccesary columns\n",
    "df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)\n",
    "\n",
    "# Change all column headings to be lower case, and remove spacing\n",
    "df.columns = [str(x).lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.618266Z",
     "start_time": "2019-11-29T13:05:19.605808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to file\n",
    "df.to_csv(cwd+config['data_processed_path'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write job to extract latest price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:19.627165Z",
     "start_time": "2019-11-29T13:05:19.621164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./extract.py\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "def parse(stock_code):\n",
    "    \"\"\"\n",
    "    Parse yahoo finance webpage\n",
    "    :return:\n",
    "    stock year low, stock year high, stock price\n",
    "    \"\"\"\n",
    "    url = \"https://finance.yahoo.com/quote/%s\" % (stock_code)\n",
    "    response = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # Find current price\n",
    "    y = soup.findAll('span', attrs={'class': 'Trsdu(0.3s) Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(b)', 'data-reactid': \"14\"})[0]\n",
    "\n",
    "    return float(y.text.replace(',', ''))\n",
    "\n",
    "def insert(path, date, price):\n",
    "    \"\"\"\n",
    "    Insert new data into the data file specified by path\n",
    "    \"\"\"\n",
    "    with open(path, 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow([str(date.today()), str(price)])\n",
    "    \n",
    "# Get latest price\n",
    "price = parse(config['stock_code'])\n",
    "\n",
    "# Insert into data file\n",
    "insert(cwd+config['data_processed_path'], date, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:22.443738Z",
     "start_time": "2019-11-29T13:05:19.629438Z"
    }
   },
   "outputs": [],
   "source": [
    "!python extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:22.453063Z",
     "start_time": "2019-11-29T13:05:22.447454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./requirements.txt\n",
    "beautifulsoup4==4.7.1\n",
    "joblib==0.13.2\n",
    "pandas==0.24.2\n",
    "pyyaml==5.1.1\n",
    "xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:56:38.238480Z",
     "start_time": "2019-11-14T08:56:38.235230Z"
    }
   },
   "source": [
    "# Write model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:22.465442Z",
     "start_time": "2019-11-29T13:05:22.455987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model.py\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "class Xgboost_model:\n",
    "    def __init__(self, N, H):\n",
    "        \"\"\"\n",
    "        Initialize model.\n",
    "        Inputs\n",
    "            N: For feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "            H: Forecast horizon, in days. Note there are about 252 trading days in a year\n",
    "        \"\"\"\n",
    "        # Load model. This is what you get when you do joblib.dump(model, \"weights.pkl\")\n",
    "        self.model = joblib.load(cwd+\"/weights.pkl\")\n",
    "\n",
    "        # Load parameters\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "\n",
    "        # Get list of features\n",
    "        self.features = []\n",
    "        for n in range(self.N, 0, -1):\n",
    "            self.features.append(\"adj_close_lag_\"+str(n))\n",
    "\n",
    "    def add_lags(self, df, N, lag_cols):\n",
    "        \"\"\"\n",
    "        Add lags up to N number of days to use as features\n",
    "        The lag columns are labelled as 'adj_close_lag_1', 'adj_close_lag_2', ... etc.\n",
    "        \"\"\"\n",
    "        # Use lags up to N number of days to use as features\n",
    "        df_w_lags = df.copy()\n",
    "        # Add a column 'order_day' to indicate the order of the rows by date\n",
    "        df_w_lags.loc[:, 'order_day'] = [x for x in list(range(len(df)))]\n",
    "        merging_keys = ['order_day']  # merging_keys\n",
    "        shift_range = [x+1 for x in range(N)]\n",
    "        for shift in shift_range:\n",
    "            train_shift = df_w_lags[merging_keys + lag_cols].copy()\n",
    "\n",
    "            # E.g. order_day of 0 becomes 1, for shift = 1.\n",
    "            # So when this is merged with order_day of 1 in df_w_lags, this will represent lag of 1.\n",
    "            train_shift['order_day'] = train_shift['order_day'] + shift\n",
    "\n",
    "            def foo(x): return '{}_lag_{}'.format(\n",
    "                x, shift) if x in lag_cols else x\n",
    "            train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "            df_w_lags = pd.merge(df_w_lags, train_shift,\n",
    "                                 on=merging_keys, how='left')  # .fillna(0)\n",
    "        del train_shift\n",
    "\n",
    "        return df_w_lags\n",
    "\n",
    "    def do_scaling(self, df, N):\n",
    "        \"\"\"\n",
    "        Do scaling for the adj_close and lag cols\n",
    "        \"\"\"\n",
    "        df.loc[:, 'adj_close_scaled'] = (\n",
    "            df['adj_close'] - df['adj_close_mean']) / df['adj_close_std']\n",
    "        for n in range(N, 0, -1):\n",
    "            df.loc[:, 'adj_close_scaled_lag_'+str(n)] = \\\n",
    "                (df['adj_close_lag_'+str(n)] - df['adj_close_mean']) / df['adj_close_std']\n",
    "\n",
    "            # Remove adj_close_lag column which we don't need anymore\n",
    "            df.drop(['adj_close_lag_'+str(n)], axis=1, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_mov_avg_std(self, df, col, N):\n",
    "        \"\"\"\n",
    "        Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "        Inputs\n",
    "            df         : dataframe. Can be of any length.\n",
    "            col        : name of the column you want to calculate mean and std dev\n",
    "            N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "        Outputs\n",
    "            df_out     : same as df but with additional column containing mean and std dev\n",
    "        \"\"\"\n",
    "        mean_list = df[col].rolling(\n",
    "            window=N, min_periods=1).mean()  # len(mean_list) = len(df)\n",
    "        # first value will be NaN, because normalized by N-1\n",
    "        std_list = df[col].rolling(window=N, min_periods=1).std()\n",
    "\n",
    "        # Add one timestep to the predictions\n",
    "        mean_list = np.concatenate(\n",
    "            (np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "        std_list = np.concatenate(\n",
    "            (np.array([np.nan]), np.array(std_list[:-1])))\n",
    "\n",
    "        # Append mean_list to df\n",
    "        df_out = df.copy()\n",
    "        df_out[col + '_mean'] = mean_list\n",
    "        df_out[col + '_std'] = std_list\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Do transformation on data\n",
    "        \"\"\"\n",
    "        data0 = data.copy(deep=True)\n",
    "        \n",
    "        # Add lags up to N number of days to use as features\n",
    "        data0 = self.add_lags(data0, self.N, ['adj_close'])\n",
    "\n",
    "        # Get mean and std dev at timestamp t using values from t-1, ..., t-N\n",
    "        data0 = self.get_mov_avg_std(data0, 'adj_close', self.N)\n",
    "\n",
    "        # Do scaling\n",
    "        data0 = self.do_scaling(data0, self.N)\n",
    "        \n",
    "        # Drop the NaNs\n",
    "        data0.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "        return data0\n",
    "\n",
    "    def pred_xgboost(self, model, N, H, prev_vals, prev_mean_val, prev_std_val):\n",
    "        \"\"\"\n",
    "        Do recursive forecasting using xgboost\n",
    "        Inputs\n",
    "            model              : the xgboost model\n",
    "            N                  : for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "            H                  : forecast horizon\n",
    "            prev_vals          : numpy array. If predict at time t, \n",
    "                                 prev_vals will contain the N unscaled values at t-1, t-2, ..., t-N\n",
    "            prev_mean_val      : the mean of the unscaled values at t-1, t-2, ..., t-N\n",
    "            prev_std_val       : the std deviation of the unscaled values at t-1, t-2, ..., t-N\n",
    "        Outputs\n",
    "            Times series of predictions. Numpy array of shape (H,). This is unscaled.\n",
    "        \"\"\"\n",
    "        forecast = prev_vals.copy()\n",
    "\n",
    "        for n in range(H):\n",
    "            forecast_scaled = (forecast[-N:] - prev_mean_val) / prev_std_val\n",
    "\n",
    "            # Create the features dataframe\n",
    "            X = defaultdict(list)\n",
    "            for n in range(N, 0, -1):\n",
    "                X['adj_close_scaled_lag_'+str(n)] = [forecast_scaled[-n]]\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "            # Do prediction\n",
    "            est_scaled = self.model.predict(X)\n",
    "\n",
    "            # Unscale the prediction\n",
    "            forecast = np.concatenate([forecast,\n",
    "                                       np.array((est_scaled * prev_std_val) + prev_mean_val).reshape(1,)])\n",
    "\n",
    "            # Comp. new mean and std\n",
    "            prev_mean_val = np.mean(forecast[-N:])\n",
    "            prev_std_val = np.std(forecast[-N:])\n",
    "\n",
    "        return forecast[-H:]\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Do predictions\n",
    "        \"\"\"\n",
    "        prev_vals = df[-self.N:]['adj_close'].to_numpy()\n",
    "        prev_mean_val = np.mean(prev_vals)\n",
    "        prev_std_val = np.std(prev_vals)\n",
    "\n",
    "        # Get predicted labels and scale back to original range\n",
    "        est = self.pred_xgboost(self.model, self.N, self.H, prev_vals,\n",
    "                                prev_mean_val, prev_std_val)\n",
    "\n",
    "        return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:22.591635Z",
     "start_time": "2019-11-29T13:05:22.468396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: out: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:22.601946Z",
     "start_time": "2019-11-29T13:05:22.594480Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./predict.py\n",
    "\n",
    "import model as mod\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from datetime import date\n",
    "from numpy import savetxt\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Load config\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Load test file\n",
    "data = pd.read_csv(cwd+config['data_processed_path'], sep=\",\")\n",
    "\n",
    "# Create an instance of xgboost_model\n",
    "xgb_model = mod.Xgboost_model(N=10, H=21)\n",
    "\n",
    "# Do transformation\n",
    "data = xgb_model.transform(data)\n",
    "\n",
    "# Do prediction\n",
    "est = xgb_model.predict(data)\n",
    "\n",
    "# Save predictions to file\n",
    "savetxt(cwd+'/out/est_' + str(date.today()) + '.csv', est, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:25.618609Z",
     "start_time": "2019-11-29T13:05:22.607789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the prediction script\n",
    "!python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:25.640210Z",
     "start_time": "2019-11-29T13:05:25.625721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.596570281982421875e+02',\n",
       " '1.590828552246093750e+02',\n",
       " '1.591213989257812500e+02',\n",
       " '1.588684539794921875e+02',\n",
       " '1.595322875976562500e+02',\n",
       " '1.592567596435546875e+02',\n",
       " '1.595471191406250000e+02',\n",
       " '1.595873870849609375e+02',\n",
       " '1.597397155761718750e+02',\n",
       " '1.601928558349609375e+02',\n",
       " '1.601864318847656250e+02',\n",
       " '1.601270446777343750e+02',\n",
       " '1.601215057373046875e+02',\n",
       " '1.601946411132812500e+02',\n",
       " '1.602317962646484375e+02',\n",
       " '1.602755584716796875e+02',\n",
       " '1.603481292724609375e+02',\n",
       " '1.603082427978515625e+02',\n",
       " '1.602938079833984375e+02',\n",
       " '1.602314300537109375e+02',\n",
       " '1.602135467529296875e+02']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output\n",
    "import csv\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "est2 = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est2.append(row[0])\n",
    "est2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:25.648887Z",
     "start_time": "2019-11-29T13:05:25.642826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./plot.py\n",
    "import chart_studio.plotly as py\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def gen_plotly_url():\n",
    "    # Sign in to plotly if you haven't done so\n",
    "    py.sign_in('<YOUR-PLOTLY-USERNAME>', '<YOUR-PLOTLY-PASSWORD>')  # Be careful with this, don't put it on Github!!!\n",
    "\n",
    "    # Get current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Load config\n",
    "    with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    # Load test file\n",
    "    df = pd.read_csv(cwd+config['data_processed_path'], sep=\",\")\n",
    "\n",
    "    # Load the predictions\n",
    "    est_dict = {'date': [df[-1:]['date'].values[0]],\n",
    "                'forecast': [df[-1:]['adj_close'].values[0]]}\n",
    "    day = 1\n",
    "    with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            est_dict['date'].append(str(date.today() + timedelta(days=day)))\n",
    "            est_dict['forecast'].append(float(row[0]))\n",
    "            day = day + 1\n",
    "    est_df = pd.DataFrame(est_dict)\n",
    "\n",
    "    # Plot with plotly\n",
    "    miny = min(min(df[-63:]['adj_close']), min(est_df['forecast']))-1 # min y-value of the plot\n",
    "    maxy = max(max(df[-63:]['adj_close']), max(est_df['forecast']))+1 # max y-value of the plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df[-63:]['date'],\n",
    "                             y=df[-63:]['adj_close'],\n",
    "                             mode='lines',\n",
    "                             name='actual',\n",
    "                             line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(x=est_df['date'],\n",
    "                             y=est_df['forecast'],\n",
    "                             mode='lines',\n",
    "                             name='predictions',\n",
    "                             line=dict(color='red')))\n",
    "    fig.add_trace(go.Scatter(x=[str(date.today()), str(date.today())],\n",
    "                             y=[miny, maxy],\n",
    "                             mode='lines',\n",
    "                             line=dict(color='black', dash=\"dot\"),\n",
    "                             showlegend=False))\n",
    "    fig.update_layout(yaxis=dict(title='USD'),\n",
    "                      xaxis=dict(title='date'))\n",
    "    fig.update_yaxes(range=[miny, maxy])\n",
    "    \n",
    "    url=py.plot(fig, filename='est_'+str(date.today()), auto_open=False)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:34.922663Z",
     "start_time": "2019-11-29T13:05:25.651048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://plot.ly/~ngyibin/377/\n"
     ]
    }
   ],
   "source": [
    "# Run the plot script\n",
    "import plot as pl\n",
    "\n",
    "url = pl.gen_plotly_url()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the plotly image is generated at the above url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:34.934028Z",
     "start_time": "2019-11-29T13:05:34.926012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://plot.ly/~ngyibin/377/\" target=\"_blank\"><img src=\"https://plot.ly/~ngyibin/377/.png\"></a><br><a href=\"https://plot.ly/~ngyibin/377/\" style=\"color: rgb(190,190,190); text-decoration: none; font-weight: 200;\" target=\"_blank\">Click to comment and see the interactive graph</a><br><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check HTML scipt for plotly plot\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "graphs = [url]\n",
    "\n",
    "template = (''\n",
    "    '<a href=\"{graph_url}\" target=\"_blank\">' # Open the interactive graph when you click on the image\n",
    "        '<img src=\"{graph_url}.png\">'        # Use the \".png\" magic url so that the latest, most-up-to-date image is included\n",
    "    '</a>'\n",
    "    '{caption}'                              # Optional caption to include below the graph\n",
    "    '<br>'                                   # Line break\n",
    "    '<a href=\"{graph_url}\" style=\"color: rgb(190,190,190); text-decoration: none; font-weight: 200;\" target=\"_blank\">'\n",
    "        'Click to comment and see the interactive graph'  # Direct readers to Plotly for commenting, interactive graph\n",
    "    '</a>'\n",
    "    '<br>'\n",
    "    '<hr>'                                   # horizontal line\n",
    "'')\n",
    "\n",
    "email_body = ''\n",
    "for graph in graphs:\n",
    "    _ = template\n",
    "    _ = _.format(graph_url=graph, caption='')\n",
    "    email_body += _\n",
    "\n",
    "display(HTML(email_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:51.265807Z",
     "start_time": "2019-11-18T05:27:48.348281Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import smtplib\n",
    "\n",
    "from datetime import date\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "TO_LIST=[\"<TO-EMAIL-ADDRESSES>\"]\n",
    "FROM_EMAIL=\"<YOUR-EMAIL-ADDRESS>\"\n",
    "\n",
    "est2 = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est2.append(row[0])\n",
    "\n",
    "def sendEmail(subj, body):\n",
    "    msg = MIMEText(str(body))\n",
    "    msg['Subject'] = subj\n",
    "    msg['From'] = FROM_EMAIL\n",
    "    \n",
    "    s = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    s.login(\"<YOUR-USER-NAME>\", \"<YOUR-PASSWORD>\") # Enter your username and password here. Be careful with this, don't put it on Github!!!\n",
    "    s.sendmail(FROM_EMAIL, TO_LIST, msg.as_string())\n",
    "    s.quit()\n",
    "    \n",
    "sendEmail('Results for ' + str(date.today()), est2)\n",
    "# Check you got the email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write send_email file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:25:34.627078Z",
     "start_time": "2019-11-29T13:25:34.610841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./send_email.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./send_email.py\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import plot as pl\n",
    "import smtplib\n",
    "import email.utils\n",
    "import yaml\n",
    "from datetime import date\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Load config\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Replace sender@example.com with your \"From\" address.\n",
    "# This address must be verified.\n",
    "SENDER = '<YOUR-EMAIL-ADDRESS>'\n",
    "SENDERNAME = '<YOUR-COMPANYS-NAME>'\n",
    "\n",
    "# Replace recipient@example.com with a \"To\" address. If your account\n",
    "# is still in the sandbox, this address must be verified.\n",
    "RECIPIENT = ['<TO-EMAIL-ADDRESS1>', '<TO-EMAIL-ADDRESS2>']\n",
    "\n",
    "# Replace smtp_username with your Amazon SES SMTP user name.\n",
    "USERNAME_SMTP = \"<SMTP-USERNAME>\" # Be careful with this, don't put it on Github!!!\n",
    "\n",
    "# Replace smtp_password with your Amazon SES SMTP password.\n",
    "PASSWORD_SMTP = \"<SMTP-PASSWORD>\" # Be careful with this, don't put it on Github!!!\n",
    "\n",
    "# If you're using Amazon SES in an AWS Region other than US West (Oregon),\n",
    "# replace email-smtp.us-west-2.amazonaws.com with the Amazon SES SMTP\n",
    "# endpoint in the appropriate region.\n",
    "HOST = \"email-smtp.us-west-2.amazonaws.com\"\n",
    "PORT = 587\n",
    "\n",
    "# Load predictions into a list\n",
    "est = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est.append(row[0])\n",
    "\n",
    "# The subject line of the email.\n",
    "SUBJECT = 'Results for ' + str(date.today())\n",
    "\n",
    "# Load test file\n",
    "data = pd.read_csv(cwd+config['data_processed_path'], sep=\",\")\n",
    "\n",
    "# Run the plot script\n",
    "url = pl.gen_plotly_url()\n",
    "\n",
    "# The email body for recipients with non-HTML email clients.\n",
    "BODY_TEXT = (\"Current price: \" + \n",
    "             str(data[-1:]['adj_close'].values) + \n",
    "             \"\\n Forecast for next 21 days using XGBoost: \" +\n",
    "             str(est))\n",
    "\n",
    "# The HTML body of the email.\n",
    "BODY_HTML = \"\"\"<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {{\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}}\n",
    "th, td {{\n",
    "  padding: 15px;\n",
    "}}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Current price</h1>\n",
    "  <p>\"\"\" + \"{0:.2f}\".format(float(data[-1:]['adj_close'].values)) + \"\"\"</p>\n",
    "  <h1>Forecast for next 21 days using XGBoost</h1>\n",
    "  <table>\n",
    "    <tr><th>Day</th><th>Forecast</th></tr>\n",
    "    <tr><td>1</td><td>\"\"\" + \"{0:.2f}\".format(float(est[0])) + \"\"\"</td></tr>\n",
    "    <tr><td>2</td><td>\"\"\" + \"{0:.2f}\".format(float(est[1])) + \"\"\"</td></tr>\n",
    "    <tr><td>3</td><td>\"\"\" + \"{0:.2f}\".format(float(est[2])) + \"\"\"</td></tr>\n",
    "    <tr><td>4</td><td>\"\"\" + \"{0:.2f}\".format(float(est[3])) + \"\"\"</td></tr>\n",
    "    <tr><td>5</td><td>\"\"\" + \"{0:.2f}\".format(float(est[4])) + \"\"\"</td></tr>\n",
    "    <tr><td>6</td><td>\"\"\" + \"{0:.2f}\".format(float(est[5])) + \"\"\"</td></tr>\n",
    "    <tr><td>7</td><td>\"\"\" + \"{0:.2f}\".format(float(est[6])) + \"\"\"</td></tr>\n",
    "    <tr><td>8</td><td>\"\"\" + \"{0:.2f}\".format(float(est[7])) + \"\"\"</td></tr>\n",
    "    <tr><td>9</td><td>\"\"\" + \"{0:.2f}\".format(float(est[8])) + \"\"\"</td></tr>\n",
    "    <tr><td>10</td><td>\"\"\" + \"{0:.2f}\".format(float(est[9])) + \"\"\"</td></tr>\n",
    "    <tr><td>11</td><td>\"\"\" + \"{0:.2f}\".format(float(est[10])) + \"\"\"</td></tr>\n",
    "    <tr><td>12</td><td>\"\"\" + \"{0:.2f}\".format(float(est[11])) + \"\"\"</td></tr>\n",
    "    <tr><td>13</td><td>\"\"\" +\"{0:.2f}\".format(float(est[12])) + \"\"\"</td></tr>\n",
    "    <tr><td>14</td><td>\"\"\" + \"{0:.2f}\".format(float(est[13])) + \"\"\"</td></tr>\n",
    "    <tr><td>15</td><td>\"\"\" + \"{0:.2f}\".format(float(est[14])) + \"\"\"</td></tr>\n",
    "    <tr><td>16</td><td>\"\"\" + \"{0:.2f}\".format(float(est[15])) + \"\"\"</td></tr>\n",
    "    <tr><td>17</td><td>\"\"\" + \"{0:.2f}\".format(float(est[16])) + \"\"\"</td></tr>\n",
    "    <tr><td>18</td><td>\"\"\" + \"{0:.2f}\".format(float(est[17])) + \"\"\"</td></tr>\n",
    "    <tr><td>19</td><td>\"\"\" + \"{0:.2f}\".format(float(est[18])) + \"\"\"</td></tr>\n",
    "    <tr><td>20</td><td>\"\"\" + \"{0:.2f}\".format(float(est[19])) + \"\"\"</td></tr>\n",
    "    <tr><td>21</td><td>\"\"\" + \"{0:.2f}\".format(float(est[20])) + \"\"\"</td></tr>\n",
    "  </table>\n",
    "  <a href=\"{graph_url}\" target=\"_blank\">\n",
    "    <img src=\"{graph_url}.png\">\n",
    "  </a>\n",
    "  {caption}\n",
    "  <br>\n",
    "  <a href=\"{graph_url}\" style=\"color: rgb(190,190,190); text-decoration: none; font-weight: 200;\" target=\"_blank\">\n",
    "    Click to comment and see the interactive graph  \n",
    "  </a>\n",
    "  <br>\n",
    "  <hr>  \n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "BODY_HTML = BODY_HTML.format(graph_url=url, caption='')\n",
    "\n",
    "# Create message container - the correct MIME type is multipart/alternative.\n",
    "msg = MIMEMultipart('alternative')\n",
    "msg['Subject'] = SUBJECT\n",
    "msg['From'] = email.utils.formataddr((SENDERNAME, SENDER))\n",
    "msg['To'] = \", \".join(RECIPIENT)\n",
    "# Comment or delete the next line if you are not using a configuration set\n",
    "#msg.add_header('X-SES-CONFIGURATION-SET',CONFIGURATION_SET)\n",
    "\n",
    "# Record the MIME types of both parts - text/plain and text/html.\n",
    "part1 = MIMEText(BODY_TEXT, 'plain')\n",
    "part2 = MIMEText(BODY_HTML, 'html')\n",
    "\n",
    "# Attach parts into message container.\n",
    "# According to RFC 2046, the last part of a multipart message, in this case\n",
    "# the HTML message, is best and preferred.\n",
    "msg.attach(part1)\n",
    "msg.attach(part2)\n",
    "\n",
    "# Try to send the message.\n",
    "try:\n",
    "    server = smtplib.SMTP(HOST, PORT)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    #stmplib docs recommend calling ehlo() before & after starttls()\n",
    "    server.ehlo()\n",
    "    server.login(USERNAME_SMTP, PASSWORD_SMTP)\n",
    "    server.sendmail(SENDER, RECIPIENT, msg.as_string())\n",
    "    server.close()\n",
    "# Display an error message if something goes wrong.\n",
    "except Exception as e:\n",
    "    print (\"Error: \", e)\n",
    "else:\n",
    "    print (\"Email sent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
