{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Productionize maching learning model using weights.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.402092Z",
     "start_time": "2019-11-18T05:27:42.392052Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.429600Z",
     "start_time": "2019-11-18T05:27:42.405610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yibin/Documents/Stocks/StockPricePrediction_deployment/xgboost_model'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current working directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.570449Z",
     "start_time": "2019-11-18T05:27:42.432213Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.584715Z",
     "start_time": "2019-11-18T05:27:42.576393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./config/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/config.yml\n",
    "data_downloaded_path: \"/data/VTI_downloaded.csv\" # specify the path of your downloaded file here\n",
    "data_processed_path: \"/data/VTI.csv\"             # specify the path of your processed file here\n",
    "stock_code: \"VTI\"                                # stock code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.595919Z",
     "start_time": "2019-11-18T05:27:42.587293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_downloaded_path': '/data/VTI_downloaded.csv',\n",
       " 'data_processed_path': '/data/VTI.csv',\n",
       " 'stock_code': 'VTI'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load config file\n",
    "import yaml\n",
    "\n",
    "with open(\"./config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data into right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.627754Z",
     "start_time": "2019-11-18T05:27:42.597933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-11-17</td>\n",
       "      <td>95.533783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>96.097031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-19</td>\n",
       "      <td>95.851738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>96.124275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-11-21</td>\n",
       "      <td>96.633034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  adj_close\n",
       "0  2014-11-17  95.533783\n",
       "1  2014-11-18  96.097031\n",
       "2  2014-11-19  95.851738\n",
       "3  2014-11-20  96.124275\n",
       "4  2014-11-21  96.633034"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(cwd+config['data_downloaded_path'])\n",
    "\n",
    "# Remove unneccesary columns\n",
    "df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)\n",
    "\n",
    "# Change all column headings to be lower case, and remove spacing\n",
    "df.columns = [str(x).lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:42.644155Z",
     "start_time": "2019-11-18T05:27:42.632838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to file\n",
    "df.to_csv(cwd+config['data_processed_path'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write job to extract latest price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:46:03.962302Z",
     "start_time": "2019-11-18T05:46:03.955100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./extract.py\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "def parse(stock_code):\n",
    "    \"\"\"\n",
    "    Parse yahoo finance webpage\n",
    "    :return:\n",
    "    stock year low, stock year high, stock price\n",
    "    \"\"\"\n",
    "    url = \"https://finance.yahoo.com/quote/%s\" % (stock_code)\n",
    "    response = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # Find current price\n",
    "    y = soup.findAll('span', attrs={'class': 'Trsdu(0.3s) Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(b)', 'data-reactid': \"14\"})[0]\n",
    "\n",
    "    return float(y.text.replace(',', ''))\n",
    "\n",
    "def insert(path, date, price):\n",
    "    \"\"\"\n",
    "    Insert new data into the data file specified by path\n",
    "    \"\"\"\n",
    "    with open(path, 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow([str(date.today()), str(price)])\n",
    "    \n",
    "# Get latest price\n",
    "price = parse(config['stock_code'])\n",
    "\n",
    "# Insert into data file\n",
    "insert(cwd+config['data_processed_path'], date, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:46:08.955571Z",
     "start_time": "2019-11-18T05:46:07.260977Z"
    }
   },
   "outputs": [],
   "source": [
    "!python extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:45.708096Z",
     "start_time": "2019-11-18T05:27:45.698771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./requirements.txt\n",
    "beautifulsoup4==4.7.1\n",
    "joblib==0.13.2\n",
    "pandas==0.24.2\n",
    "pyyaml==5.1.1\n",
    "xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:56:38.238480Z",
     "start_time": "2019-11-14T08:56:38.235230Z"
    }
   },
   "source": [
    "# Write model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:45.718871Z",
     "start_time": "2019-11-18T05:27:45.712237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model.py\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "class Xgboost_model:\n",
    "    def __init__(self, N, H):\n",
    "        \"\"\"\n",
    "        Initialize model.\n",
    "        Inputs\n",
    "            N: For feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "            H: Forecast horizon, in days. Note there are about 252 trading days in a year\n",
    "        \"\"\"\n",
    "        # Load model. This is what you get when you do joblib.dump(model, \"weights.pkl\")\n",
    "        self.model = joblib.load(cwd+\"/weights.pkl\")\n",
    "\n",
    "        # Load parameters\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "\n",
    "        # Get list of features\n",
    "        self.features = []\n",
    "        for n in range(self.N, 0, -1):\n",
    "            self.features.append(\"adj_close_lag_\"+str(n))\n",
    "\n",
    "    def add_lags(self, df, N, lag_cols):\n",
    "        \"\"\"\n",
    "        Add lags up to N number of days to use as features\n",
    "        The lag columns are labelled as 'adj_close_lag_1', 'adj_close_lag_2', ... etc.\n",
    "        \"\"\"\n",
    "        # Use lags up to N number of days to use as features\n",
    "        df_w_lags = df.copy()\n",
    "        # Add a column 'order_day' to indicate the order of the rows by date\n",
    "        df_w_lags.loc[:, 'order_day'] = [x for x in list(range(len(df)))]\n",
    "        merging_keys = ['order_day']  # merging_keys\n",
    "        shift_range = [x+1 for x in range(N)]\n",
    "        for shift in shift_range:\n",
    "            train_shift = df_w_lags[merging_keys + lag_cols].copy()\n",
    "\n",
    "            # E.g. order_day of 0 becomes 1, for shift = 1.\n",
    "            # So when this is merged with order_day of 1 in df_w_lags, this will represent lag of 1.\n",
    "            train_shift['order_day'] = train_shift['order_day'] + shift\n",
    "\n",
    "            def foo(x): return '{}_lag_{}'.format(\n",
    "                x, shift) if x in lag_cols else x\n",
    "            train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "            df_w_lags = pd.merge(df_w_lags, train_shift,\n",
    "                                 on=merging_keys, how='left')  # .fillna(0)\n",
    "        del train_shift\n",
    "\n",
    "        return df_w_lags\n",
    "\n",
    "    def do_scaling(self, df, N):\n",
    "        \"\"\"\n",
    "        Do scaling for the adj_close and lag cols\n",
    "        \"\"\"\n",
    "        df.loc[:, 'adj_close_scaled'] = (\n",
    "            df['adj_close'] - df['adj_close_mean']) / df['adj_close_std']\n",
    "        for n in range(N, 0, -1):\n",
    "            df.loc[:, 'adj_close_scaled_lag_'+str(n)] = \\\n",
    "                (df['adj_close_lag_'+str(n)] - df['adj_close_mean']) / df['adj_close_std']\n",
    "\n",
    "            # Remove adj_close_lag column which we don't need anymore\n",
    "            df.drop(['adj_close_lag_'+str(n)], axis=1, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_mov_avg_std(self, df, col, N):\n",
    "        \"\"\"\n",
    "        Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "        Inputs\n",
    "            df         : dataframe. Can be of any length.\n",
    "            col        : name of the column you want to calculate mean and std dev\n",
    "            N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "        Outputs\n",
    "            df_out     : same as df but with additional column containing mean and std dev\n",
    "        \"\"\"\n",
    "        mean_list = df[col].rolling(\n",
    "            window=N, min_periods=1).mean()  # len(mean_list) = len(df)\n",
    "        # first value will be NaN, because normalized by N-1\n",
    "        std_list = df[col].rolling(window=N, min_periods=1).std()\n",
    "\n",
    "        # Add one timestep to the predictions\n",
    "        mean_list = np.concatenate(\n",
    "            (np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "        std_list = np.concatenate(\n",
    "            (np.array([np.nan]), np.array(std_list[:-1])))\n",
    "\n",
    "        # Append mean_list to df\n",
    "        df_out = df.copy()\n",
    "        df_out[col + '_mean'] = mean_list\n",
    "        df_out[col + '_std'] = std_list\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Do transformation on data\n",
    "        \"\"\"\n",
    "        data0 = data.copy(deep=True)\n",
    "        \n",
    "        # Add lags up to N number of days to use as features\n",
    "        data0 = self.add_lags(data0, self.N, ['adj_close'])\n",
    "\n",
    "        # Get mean and std dev at timestamp t using values from t-1, ..., t-N\n",
    "        data0 = self.get_mov_avg_std(data0, 'adj_close', self.N)\n",
    "\n",
    "        # Do scaling\n",
    "        data0 = self.do_scaling(data0, self.N)\n",
    "        \n",
    "        # Drop the NaNs\n",
    "        data0.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "        return data0\n",
    "\n",
    "    def pred_xgboost(self, model, N, H, prev_vals, prev_mean_val, prev_std_val):\n",
    "        \"\"\"\n",
    "        Do recursive forecasting using xgboost\n",
    "        Inputs\n",
    "            model              : the xgboost model\n",
    "            N                  : for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "            H                  : forecast horizon\n",
    "            prev_vals          : numpy array. If predict at time t, \n",
    "                                 prev_vals will contain the N unscaled values at t-1, t-2, ..., t-N\n",
    "            prev_mean_val      : the mean of the unscaled values at t-1, t-2, ..., t-N\n",
    "            prev_std_val       : the std deviation of the unscaled values at t-1, t-2, ..., t-N\n",
    "        Outputs\n",
    "            Times series of predictions. Numpy array of shape (H,). This is unscaled.\n",
    "        \"\"\"\n",
    "        forecast = prev_vals.copy()\n",
    "\n",
    "        for n in range(H):\n",
    "            forecast_scaled = (forecast[-N:] - prev_mean_val) / prev_std_val\n",
    "\n",
    "            # Create the features dataframe\n",
    "            X = defaultdict(list)\n",
    "            for n in range(N, 0, -1):\n",
    "                X['adj_close_scaled_lag_'+str(n)] = [forecast_scaled[-n]]\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "            # Do prediction\n",
    "            est_scaled = self.model.predict(X)\n",
    "\n",
    "            # Unscale the prediction\n",
    "            forecast = np.concatenate([forecast,\n",
    "                                       np.array((est_scaled * prev_std_val) + prev_mean_val).reshape(1,)])\n",
    "\n",
    "            # Comp. new mean and std\n",
    "            prev_mean_val = np.mean(forecast[-N:])\n",
    "            prev_std_val = np.std(forecast[-N:])\n",
    "\n",
    "        return forecast[-H:]\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Do predictions\n",
    "        \"\"\"\n",
    "        prev_vals = df[-self.N:]['adj_close'].to_numpy()\n",
    "        prev_mean_val = np.mean(prev_vals)\n",
    "        prev_std_val = np.std(prev_vals)\n",
    "\n",
    "        # Get predicted labels and scale back to original range\n",
    "        est = self.pred_xgboost(self.model, self.N, self.H, prev_vals,\n",
    "                                prev_mean_val, prev_std_val)\n",
    "\n",
    "        return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:45.839861Z",
     "start_time": "2019-11-18T05:27:45.721496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: out: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:45.850061Z",
     "start_time": "2019-11-18T05:27:45.843198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./predict.py\n",
    "\n",
    "import model as mod\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from datetime import date\n",
    "from numpy import savetxt\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Load config\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Load test file\n",
    "data = pd.read_csv(cwd+config['data_processed_path'], sep=\",\")\n",
    "\n",
    "# Create an instance of xgboost_model\n",
    "xgb_model = mod.Xgboost_model(N=10, H=21)\n",
    "\n",
    "# Do transformation\n",
    "data = xgb_model.transform(data)\n",
    "\n",
    "# Do prediction\n",
    "est = xgb_model.predict(data)\n",
    "\n",
    "# Save predictions to file\n",
    "savetxt(cwd+'/out/est_' + str(date.today()) + '.csv', est, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:48.325720Z",
     "start_time": "2019-11-18T05:27:45.853900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the prediction script\n",
    "!python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:48.345942Z",
     "start_time": "2019-11-18T05:27:48.331822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.581709899902343750e+02',\n",
       " '1.582647705078125000e+02',\n",
       " '1.583576049804687500e+02',\n",
       " '1.585061340332031250e+02',\n",
       " '1.587239532470703125e+02',\n",
       " '1.587457580566406250e+02',\n",
       " '1.587290802001953125e+02',\n",
       " '1.586892852783203125e+02',\n",
       " '1.587031402587890625e+02',\n",
       " '1.586796112060546875e+02',\n",
       " '1.587208557128906250e+02',\n",
       " '1.587702941894531250e+02',\n",
       " '1.586327514648437500e+02',\n",
       " '1.586725616455078125e+02',\n",
       " '1.585801544189453125e+02',\n",
       " '1.585864715576171875e+02',\n",
       " '1.585883026123046875e+02',\n",
       " '1.585937194824218750e+02',\n",
       " '1.585939941406250000e+02',\n",
       " '1.586228790283203125e+02',\n",
       " '1.586268768310546875e+02']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output\n",
    "import csv\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "est2 = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est2.append(row[0])\n",
    "est2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T05:27:51.265807Z",
     "start_time": "2019-11-18T05:27:48.348281Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import smtplib\n",
    "\n",
    "from datetime import date\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "TO_LIST=[\"<TO-EMAIL-ADDRESSES>\"]\n",
    "FROM_EMAIL=\"<YOUR-EMAIL-ADDRESS>\"\n",
    "\n",
    "est2 = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est2.append(row[0])\n",
    "\n",
    "def sendEmail(subj, body):\n",
    "    msg = MIMEText(str(body))\n",
    "    msg['Subject'] = subj\n",
    "    msg['From'] = FROM_EMAIL\n",
    "    \n",
    "    s = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    s.login(\"<YOUR-USER-NAME>\", \"<YOUR-PASSWORD>\") # Enter your username and password here. Be careful with this, don't put it on Github!!!\n",
    "    s.sendmail(FROM_EMAIL, TO_LIST, msg.as_string())\n",
    "    s.quit()\n",
    "    \n",
    "sendEmail('Results for ' + str(date.today()), est2)\n",
    "# Check you got the email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write send_email file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T04:07:49.022009Z",
     "start_time": "2019-11-21T04:07:49.009359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./send_email.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./send_email.py\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import smtplib\n",
    "import email.utils\n",
    "import yaml\n",
    "from datetime import date\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Load config\n",
    "with open(cwd+\"/config/config.yml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Replace sender@example.com with your \"From\" address.\n",
    "# This address must be verified.\n",
    "SENDER = '<YOUR-EMAIL-ADDRESS>'\n",
    "SENDERNAME = '<YOUR-COMPANYS-NAME>'\n",
    "\n",
    "# Replace recipient@example.com with a \"To\" address. If your account\n",
    "# is still in the sandbox, this address must be verified.\n",
    "RECIPIENT  = '<TO-EMAIL-ADDRESS>'\n",
    "\n",
    "# Replace smtp_username with your Amazon SES SMTP user name.\n",
    "USERNAME_SMTP = \"<SMTP-USERNAME>\" # Be careful with this, don't put it on Github!!!\n",
    "\n",
    "# Replace smtp_password with your Amazon SES SMTP password.\n",
    "PASSWORD_SMTP = \"<SMTP-PASSWORD>\" # Be careful with this, don't put it on Github!!!\n",
    "\n",
    "# If you're using Amazon SES in an AWS Region other than US West (Oregon),\n",
    "# replace email-smtp.us-west-2.amazonaws.com with the Amazon SES SMTP\n",
    "# endpoint in the appropriate region.\n",
    "HOST = \"email-smtp.us-west-2.amazonaws.com\"\n",
    "PORT = 587\n",
    "\n",
    "# Load predictions into a list\n",
    "est = []\n",
    "with open(cwd+'/out/est_' + str(date.today()) + '.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        est.append(row[0])\n",
    "\n",
    "# The subject line of the email.\n",
    "SUBJECT = 'Results for ' + str(date.today())\n",
    "\n",
    "# Load test file\n",
    "data = pd.read_csv(cwd+config['data_processed_path'], sep=\",\")\n",
    "\n",
    "# The email body for recipients with non-HTML email clients.\n",
    "BODY_TEXT = (\"Current price: \" + \n",
    "             str(data[-1:]['adj_close'].values) + \n",
    "             \"\\n Forecast for next 21 days using XGBoost: \" +\n",
    "             str(est))\n",
    "\n",
    "# The HTML body of the email.\n",
    "BODY_HTML = \"\"\"<html>\n",
    "<head></head>\n",
    "<body>\n",
    "  <h1>Current price</h1>\n",
    "  <p>\"\"\" + str(data[-1:]['adj_close'].values) + \"\"\"</p>\n",
    "  <h1>Forecast for next 21 days using XGBoost</h1>\n",
    "  <p>\"\"\" + str(est) + \"\"\"</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create message container - the correct MIME type is multipart/alternative.\n",
    "msg = MIMEMultipart('alternative')\n",
    "msg['Subject'] = SUBJECT\n",
    "msg['From'] = email.utils.formataddr((SENDERNAME, SENDER))\n",
    "msg['To'] = RECIPIENT\n",
    "# Comment or delete the next line if you are not using a configuration set\n",
    "#msg.add_header('X-SES-CONFIGURATION-SET',CONFIGURATION_SET)\n",
    "\n",
    "# Record the MIME types of both parts - text/plain and text/html.\n",
    "part1 = MIMEText(BODY_TEXT, 'plain')\n",
    "part2 = MIMEText(BODY_HTML, 'html')\n",
    "\n",
    "# Attach parts into message container.\n",
    "# According to RFC 2046, the last part of a multipart message, in this case\n",
    "# the HTML message, is best and preferred.\n",
    "msg.attach(part1)\n",
    "msg.attach(part2)\n",
    "\n",
    "# Try to send the message.\n",
    "try:\n",
    "    server = smtplib.SMTP(HOST, PORT)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    #stmplib docs recommend calling ehlo() before & after starttls()\n",
    "    server.ehlo()\n",
    "    server.login(USERNAME_SMTP, PASSWORD_SMTP)\n",
    "    server.sendmail(SENDER, RECIPIENT, msg.as_string())\n",
    "    server.close()f\n",
    "# Display an error message if something goes wrong.\n",
    "except Exception as e:\n",
    "    print (\"Error: \", e)\n",
    "else:\n",
    "    print (\"Email sent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
